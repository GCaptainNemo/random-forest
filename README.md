# 随机森林(random forest)
随机森林是深度学习诞生之前，最常用于作为分类、回归的模型。这里将其拆分成决策树、Bagging集成算法、随机森林三部分介绍。
## 一、决策树(decision tree)
### 1.1 介绍
以分类问题为例，在统计机器学习中，分类是求关于隐变量的后验概率(推断 inference)。而决策树则认为分类和回归是一个决策(decision)，人类在处理决策任务时是
依赖一个判定测试序列的，比如我们要判断一个西瓜是不是甜的，我们会先判断它的色泽是否是绿色的，然后判断它的根茎是什么形态，再判断它敲击的声音，当通过该判断测试序列，
我们得到了这个西瓜是不是甜的结论。将一个问题的判断测试序列表示成树的形式，即决策树。

![decision_tree](resources/decision_tree.jfif)

在决策树中，叶子节点对应决策结果，而其余节点则对应一个属性(特征)测试，从根节点到任一叶子节点的路径对应了一个判定测试序列。

### 1.2 构造决策树
给定一个训练集{(X<sub>i</sub>, Y<sub>i</sub>)} i=1, 2, 3,...,N，和Xi的属性集A = {ai} i =1, 2, ...,d，下面讨论如何构造一个合理的决策树。

#### 1.2.1 ID3算法
ID3算法的流程为：
```
1. 从根节点开始，计算根节点样本的信息熵；
2. 根据特征，计算出各个特征的信息增益；
3. 在每个节点上利用信息增益进行特征选择，选择信息增益最大的特征。
4. 如果全部是一类，则输出叶子节点，如果存在多类的情况，则需要重新寻找最优特征，直到剩余特征的信息增益小于阈值，或已经没有可以选择的特征时停止；
5. 对决策树进行剪枝优化。
```


ID3构建决策树的过程基于信息论，信息增益(Info Gain)就是信息论中的互信息:

I(X;Y) = H(X)-H(X|Y)

其中X代表决策树某节点处样本随机变量，H(X)表示样本的熵；而Y代表样本的某属性随机变量，H(X|Y)代表X关于Y的条件熵，即已知Y后，X的熵。ID3对于决策树的构建有这样的策略
—— 决策树的判断决策序列应尽快地降低样本的随机性(熵)，或者说决策树的层数应尽可能地少。这个问题是一个NP难的问题，ID3是近似求解该问题的一个贪婪算法，在每一步取信息增益最大或者说最能降低样本随机性的属性作为判断属性。
由于对总体分布未知，熵的计算通过对训练集的样本统计得到的一个经验熵。而随着决策树趋于叶子节点，相应的样本数也会减少，用来估计熵的统计量的方差会变大，发生过拟合。基于此，
需要在决策树构造之后引入减枝操作，提高模型的泛化性能。
