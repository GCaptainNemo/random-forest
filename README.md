# 随机森林(random forest)
随机森林是深度学习诞生之前，最常用于作为分类、回归的模型。这里将其拆分成决策树、Bagging集成算法、随机森林三部分介绍。
## 一、决策树(decision tree)
### 1.1 介绍
以分类问题为例，在基于概率的机器学习算法中，分类可以分为推断(inference)和决策(decision)两步。推断是求关于目标变量的后验分布P(Y|X)，而决策则是基于该后验分布给Y一个合理的估计。
此外，还有一种不引入概率的模型，即直接用一个函数将X映射到Y，该函数称为判别函数(discriminant function)，决策树就是其中的一种。

决策树认为人类在处理决策任务时依赖一个判定测试序列，比如我们要判断一个西瓜甜不甜(分类问题)，我们会先判断它的色泽是否是绿色的，然后判断它的根茎是什么形态，再判断它敲击的声音，通过该判断测试序列，
得到这个西瓜甜不甜的结论。将一个问题的判断测试序列表示成树的形式，即决策树。

![decision_tree](resources/decision_tree.jfif)

在决策树中，叶子节点对应决策结果，而其余节点则对应一个属性(特征)测试，从根节点到任一叶子节点的路径对应了一个判定测试序列。

### 1.2 构造决策树
给定一个训练集D = {(X<sub>i</sub>, Y<sub>i</sub>)} i=1, 2, 3,...,N，和Xi的属性集A = {ai} i =1, 2, ...,d，下面讨论如何构造一个合理的决策树。

#### 1.2.1 ID3决策树算法
ID3算法的流程为：
```
1. 从根节点开始，计算根节点样本的信息熵；
2. 根据特征，计算出各个特征的信息增益；
3. 在每个节点上利用信息增益进行特征选择，选择信息增益最大的特征。
4. 如果全部是一类，则输出叶子节点，如果存在多类的情况，则需要重新寻找最优特征，直到剩余特征的信息增益小于阈值，或已经没有可以选择的特征时停止；
5. 对决策树进行剪枝优化。
```


ID3构建决策树的过程基于信息论，信息增益(Info Gain)就是信息论中的互信息:

Gain(D, ai) = I(D;ai) = H(D)-H(D|ai)

a* = argmax<sub>ai</sub> I(D;ai)， ai∈A

其中D=(X, Y)代表决策树某节点处样本变量，D与Y服从相同分布，H(D)表示样本的熵；而ai代表样本的属性变量，H(D|ai)代表D关于ai的条件熵。ID3对于决策树的构建有这样的策略
—— 决策树应尽快降低样本的不确定性(熵)，或者说决策树的层数应尽可能地少，这从某种程度上说反映了奥卡姆剃刀准则。然而这个问题是一个NP难的问题，ID3是近似求解该问题的一种贪婪算法，
在每一步取信息增益最大的属性作为判断属性。由于对总体分布未知，熵的计算是通过对训练集的样本统计得到的经验熵。随着决策树趋于叶子节点，
相应样本数也会减少，用来估计熵的统计量的方差会变大，发生过拟合。因此需要在决策树构造之后引入剪枝(pruning)操作，提高模型的泛化性能。

#### 1.3 C4.5决策树算法

信息增益准对取值数目较多的属性有所偏好，为减少这种偏好的影响，C4.5决策树算法提出用增益率(gain ratio)代替信息增益，即将信息增益除以一个衡量属性ai取值数目的测度，
这种思想在图论的切图添加平衡条件也有体现。

C4.5决策树算法没有直接用属性取值的个数作为测度，而是使用了属性ai的熵作为测度：

H(ai) = ΣP(ai)logP(ai)

Gain_ratio(D, ai) = Gain(D, ai) / H(ai)

a* = argmax<sub>ai</sub> I(D;ai) / H(ai)， ai∈A

需要注意的是，使用增益率准则可能会对取值较少的属性有偏好，因此C4.5使用了一个启发式算法，即先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

#### 1.4 CART决策树
CART决策树使用基尼指数(Gini index)来选择划分属性。先介绍基尼值(Gini Value)：

Gini(D) = 1 - Σ<sub>Y</sub>P(Y)<sup>2</sup>

基尼值的意义是从变量D的分布中随机抽出两个样本，两个样本类别不一样的概率。因此熵反映了一个变量的随机性(杂乱度)，而基尼值则反映了变量的“纯净度“，值越大说明越”纯净“。基尼指数即：

Gini_index(D, ai) = Σ<sub>ai</sub>p(D|ai)Gini(D, ai)

a* =argmin<sub>ai</sub> Gini_index(D, ai) , ai∈A

#### 1.5 剪枝处理




